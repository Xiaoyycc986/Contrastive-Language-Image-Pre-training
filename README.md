# Contrastive-Language-Image-Pre-training
This repository contains concise summaries, insights, and analysis of key research papers related to CLIP (Contrastive Language‚ÄìImage Pre-training). The goal is to provide a resource for researchers and developers interested in understanding the inner workings and applications of CLIP.

## üìö Paper Summaries

| Paper Title | Link | Key Points |
|-------------|------|------------|
| Learning Transferable Visual Models From Natural Language Supervision | [arXiv](https://arxiv.org/pdf/2103.00020) | Proposes CLIP using contrastive learning with 400M image-text pairs. Strong zero-shot performance and generalization via open vocabulary. |
| Interpreting CLIP‚Äôs Image Representation via Text-based Decomposition | [arXiv](https://arxiv.org/pdf/2310.05916) | Analyzes interpretability of each transformer layer and head using text-based decomposition, identifying semantic and spatial roles. |

## üåê Framework Diagrams

### CLIP Framework
![CLIP Framework](https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true)

### CLIP Interpretability 
![CLIP Interpretability]([https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkGjH44kT-LPJSKqC9aCGw.png](https://github.com/yossigandelsman/clip_text_span/blob/main/images/teaser.png?raw=true))
